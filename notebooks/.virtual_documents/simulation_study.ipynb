


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.linalg import svd
from sklearn.decomposition import PCA
from pysva.sva import estimate_n_sv, get_residuals, sva_iterative_weighted

np.random.seed(42)





# Parameters
N = 100    # Samples
G = 5000   # Genes
TRUE_N_SV = 5

# 1. Biological Signal (Case/Control)
X = np.zeros((N, 2))
X[:, 0] = 1 # Intercept
X[50:, 1] = 1 # 50 controls, 50 cases
beta = np.zeros((2, G))
beta[1, :500] = 3.0 # Strong Signal

Signal = X @ beta

# 2. Hidden Batch Effects (True SVs)
# IMPORTANT: Make batch effects ORTHOGONAL to treatment to avoid confounding
# W is Nx5 (latent factors), alpha is 5xG (weights)
W_raw = np.random.randn(N, TRUE_N_SV)

# Orthogonalize W with respect to X (remove any correlation with treatment)
# This ensures batch effects are independent of treatment assignment
W = np.zeros_like(W_raw)
for i in range(TRUE_N_SV):
    # Regress W[:,i] on X and keep only residuals
    beta_w = np.linalg.lstsq(X, W_raw[:, i], rcond=None)[0]
    w_pred = X @ beta_w
    W[:, i] = W_raw[:, i] - w_pred  # Residuals are orthogonal to X

# Verify orthogonality
print("Correlation between batch effects and treatment:")
for i in range(TRUE_N_SV):
    corr = np.corrcoef(W[:, i], X[:, 1])[0, 1]
    print(f"  W[{i}] vs Treatment: r = {corr:.6f}")
print("✓ Batch effects are orthogonal to treatment\n")

alpha = np.random.randn(TRUE_N_SV, G) * 4 # Batch effect strength

# 2b. Sort SVs by Variance Explained (for comparison later)
# We want True SV 1 to be the strongest signal, etc.
sv_variances = []
for i in range(TRUE_N_SV):
    # Calculate component matrix for this SV
    comp = np.outer(W[:, i], alpha[i, :])
    sv_variances.append(np.var(comp))

# Sort descending
order = np.argsort(sv_variances)[::-1]
W = W[:, order]
alpha = alpha[order, :]
BatchEffect = W @ alpha

# 3. Noise
Noise = np.random.randn(N, G)

# Total Expression
Y = Signal + BatchEffect + Noise
# Y is (N, G) -> (100, 5000) (Samples x Genes)

print(f"Generated Data: {Y.shape} (Samples x Genes)")
print(f"True Number of SVs: {TRUE_N_SV}")

# Variance decomposition
print(f"\nVariance breakdown (averaged over all genes):")
print(f"  Signal:      {np.mean(np.var(Signal, axis=0)):.2f}")
print(f"  Batch:       {np.mean(np.var(BatchEffect, axis=0)):.2f}")
print(f"  Noise:       {np.mean(np.var(Noise, axis=0)):.2f}")
print(f"  Total:       {np.mean(np.var(Y, axis=0)):.2f}")





# Plot Heatmaps of Signal vs Batch
plt.figure(figsize=(15, 5))

# 1. Biological Signal Heatmap (First 1000 genes)
plt.subplot(1, 3, 1)
sns.heatmap(np.transpose(Signal[:, :1000]), cmap="RdBu_r", center=0, cbar=False)
plt.title("Biological Signal (X @ beta)\n(First 1000 Genes)")
plt.ylabel("Genes")
plt.xlabel("Samples (0-50 Ctl, 50-100 Case)")

# 2. Batch Effect Heatmap (First 1000 genes)
plt.subplot(1, 3, 2)
sns.heatmap(np.transpose(BatchEffect[:, :1000]), cmap="RdBu_r", center=0, cbar=False)
plt.title("Latent Factors (W @ alpha)\n(First 1000 Genes)")
plt.xlabel("Genes")
plt.ylabel("Samples")

# 3. Total Data Heatmap
plt.subplot(1, 3, 3)
sns.heatmap(np.transpose(Y[:, :1000]), cmap="RdBu_r", center=0)
plt.title("Total Expression (Y)\n(Signal + Batch + Noise)")
plt.xlabel("Genes")
plt.ylabel("Samples")

plt.tight_layout()
plt.show()





pca = PCA(n_components=2)
Y_pca = pca.fit_transform(Y)

plt.figure(figsize=(6, 5))
plt.scatter(Y_pca[:50, 0], Y_pca[:50, 1], label='Control', alpha=0.7)
plt.scatter(Y_pca[50:, 0], Y_pca[50:, 1], label='Case', alpha=0.7)
plt.title("PCA of Total Expression Data")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(alpha=0.3)
plt.show()





# Compute correlations between top PCs and True SVs
pca_10 = PCA(n_components=6)
pcs_10 = pca_10.fit_transform(Y)

# W is (N, 5), pcs_10 is (N, 10)
# We want to see if specific PCs align with specific SVs
n_sv = W.shape[1]
n_pc = pcs_10.shape[1]

corr_matrix = np.zeros((n_pc, n_sv))

for i in range(n_pc):
    for j in range(n_sv):
        # Pearson correlation
        corr = np.corrcoef(pcs_10[:, i], W[:, j])[0, 1]
        corr_matrix[i, j] = corr

plt.figure(figsize=(10, 8))
sns.heatmap(np.abs(corr_matrix), annot=True, fmt=".2f", cmap="viridis",
            xticklabels=[f"True SV {j+1}" for j in range(n_sv)],
            yticklabels=[f"PC {i+1}" for i in range(n_pc)])
plt.title("Absolute Correlation: Principal Components vs True Surrogate Variables")
plt.xlabel("True Latent Factors (Hidden Batch Effects)")
plt.ylabel("Principal Components (of Total Expression Y)")
plt.tight_layout()
plt.show()





residuals = get_residuals(Y, X)
U, S, Vt = svd(residuals, full_matrices=False)





# Method 1: Scree Plot
plt.figure(figsize=(10, 4))
var_prop = S**2 / np.sum(S**2)
plt.subplot(1, 2, 1)
plt.plot(var_prop[:15], 'o-', label='Observed')
plt.axvline(x=TRUE_N_SV-1, color='g', linestyle='--', label='True SVs')
plt.title("Method 1: Scree Plot")
plt.legend()
plt.grid()

# Method 2: Permutation Test
print("Running Permutation Test (this may take a moment)...")
n_sv_perm = estimate_n_sv(residuals, method="permutation", n_perm=20)
print(f"Permutation Estimate: {n_sv_perm}")

# Method 3: BIC
print("Running BIC...")
n_sv_bic = estimate_n_sv(residuals, method="bic")
print(f"BIC Estimate: {n_sv_bic}")








# Manual Permutation for Visualization
n_perm = 20
obs_prop_var = (S**2) / np.sum(S**2)
null_dist = []
n_samples, n_genes = residuals.shape

for _ in range(n_perm):
    idx = np.random.rand(n_samples, n_genes).argsort(axis=0)
    res_perm = np.take_along_axis(residuals, idx, axis=0)
    _, S_perm, _ = svd(res_perm, full_matrices=False)
    null_dist.append((S_perm**2) / np.sum(S_perm**2))

null_dist = np.array(null_dist)

# Plot
plt.figure(figsize=(10, 5))
k_plot = 15
plt.boxplot([null_dist[:, i] for i in range(k_plot)], positions=range(1, k_plot+1), widths=0.6, showfliers=False)
plt.plot(range(1, k_plot+1), obs_prop_var[:k_plot], 'ro-', label='Observed')
plt.xlabel("Component Index")
plt.ylabel("Prop. Variance Explained")
plt.title(f"Permutation Test: Observed vs Null (Truth={TRUE_N_SV})")
plt.axvline(x=TRUE_N_SV, color='g', linestyle='--', label='True n_sv')
plt.legend()
plt.grid(alpha=0.3)
plt.show()





# BIC Calculation
bics = []
max_sv_plot = 15
for k in range(max_sv_plot):
    rss = np.sum(S[k:]**2)
    n_data = N * G
    variance = rss / n_data
    if variance <= 0:
        log_lik = -np.inf
    else:
        log_lik = -0.5 * n_data * np.log(variance)
    n_params = k * (N + G - k)
    bic = -2 * log_lik + n_params * np.log(n_data)
    bics.append(bic)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(range(max_sv_plot), bics, 'o-', label='BIC Score')
plt.plot(n_sv_bic, bics[n_sv_bic], 'r*', markersize=15, label=f'Min BIC (k={n_sv_bic})')
plt.axvline(x=TRUE_N_SV, color='g', linestyle='--', label='True SVs')
plt.xlabel("Number of SVs (k)")
plt.ylabel("BIC Score")
plt.title(f"BIC Estimation (Truth={TRUE_N_SV})")
plt.legend()
plt.grid()
plt.show()





methods = ['True Truth', 'Permutation', 'BIC']
values = [TRUE_N_SV, n_sv_perm, n_sv_bic]

plt.bar(methods, values, color=['green', 'blue', 'orange'])
plt.title("Comparison of SV Estimation Methods")
plt.ylabel("Number of SVs Detected")

for i, v in enumerate(values):
    plt.text(i, v + 0.1, str(v), ha='center', fontweight='bold')

plt.show()





# Run SVA with the best estimate (BIC)
sv_est = sva_iterative_weighted(Y, X, n_sv=n_sv_bic)

# Calculate correlation matrix
cor_matrix = np.zeros((TRUE_N_SV, n_sv_bic))
for i in range(TRUE_N_SV):
    for j in range(n_sv_bic):
        # Absolute correlation because SV sign is arbitrary
        cor_matrix[i, j] = np.abs(np.corrcoef(W[:, i], sv_est[:, j])[0, 1])

# Visualize
plt.figure(figsize=(8, 6))
sns.heatmap(cor_matrix, annot=True, cmap="Reds", vmin=0, vmax=1,
            xticklabels=[f"Est SV{j+1}" for j in range(n_sv_bic)],
            yticklabels=[f"True SV{i+1}" for i in range(TRUE_N_SV)])
plt.title("Correlation: Sorted True Factors vs Estimated SVs")
plt.show()





# Regress out SVs from the data
# Model: Y ~ X + SV
# We want to remove the effect of SV, so we construct a design matrix with SV
# and subtract that component.
# Note: strictly speaking, we want to keep X. 
# The simplest way is to regress Y on [X, SV] and then subtract the SV part.

def remove_batch_effect(Y, X, sv):
    # 1. Fit full model: Y = [X, SV] @ beta_full
    design = np.hstack((X, sv))
    beta_full, _, _, _ = np.linalg.lstsq(design, Y, rcond=None)
    
    # 2. Identify which coefficients belong to SV
    n_cov = X.shape[1]
    beta_sv = beta_full[n_cov:, :]
    
    # 3. Calculate SV component
    sv_component = sv @ beta_sv
    
    # 4. Subtract SV component
    Y_adj = Y - sv_component
    return Y_adj

Y_adjusted = remove_batch_effect(Y, X, sv_est)

# PCA on Adjusted Data
pca_adj = PCA(n_components=5)
Y_pca_adj = pca_adj.fit_transform(Y_adjusted)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(Y_pca[:50, 0], Y_pca[:50, 1], label='Control', alpha=0.7)
plt.scatter(Y_pca[50:, 0], Y_pca[50:, 1], label='Case', alpha=0.7)
plt.title("Original Data (Unadjusted)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(Y_pca_adj[:50, 0], Y_pca_adj[:50, 1], label='Control', alpha=0.7)
plt.scatter(Y_pca_adj[50:, 0], Y_pca_adj[50:, 1], label='Case', alpha=0.7)
plt.title("SVA Adjusted Data")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()

plt.tight_layout()
plt.show()





from scipy import stats

# Fit linear models for each gene and extract p-values for treatment effect

# Model 1: Y ~ Treatment (without SVA)
# Model 2: Y ~ Treatment + SVs (with SVA)

n_genes = Y.shape[1]
pvals_naive = np.zeros(n_genes)
pvals_sva = np.zeros(n_genes)

print(f"Fitting linear models for {n_genes} genes...")

for gene_idx in range(n_genes):
    y_gene = Y[:, gene_idx]
    
    # Model 1: Naive (Treatment only)
    # Design: X (intercept + treatment)
    beta_naive = np.linalg.lstsq(X, y_gene, rcond=None)[0]
    y_pred_naive = X @ beta_naive
    resid_naive = y_gene - y_pred_naive
    
    # Calculate t-statistic for treatment effect (index 1)
    mse_naive = np.sum(resid_naive**2) / (N - X.shape[1])
    XtX_inv_naive = np.linalg.inv(X.T @ X)
    se_naive = np.sqrt(mse_naive * XtX_inv_naive[1, 1])
    t_stat_naive = beta_naive[1] / se_naive
    pvals_naive[gene_idx] = 2 * (1 - stats.t.cdf(np.abs(t_stat_naive), N - X.shape[1]))
    
    # Model 2: With SVA
    # Design: [X, SVs]
    X_with_sv = np.hstack([X, sv_est])
    beta_sva = np.linalg.lstsq(X_with_sv, y_gene, rcond=None)[0]
    y_pred_sva = X_with_sv @ beta_sva
    resid_sva = y_gene - y_pred_sva
    
    # Calculate t-statistic for treatment effect (still index 1)
    mse_sva = np.sum(resid_sva**2) / (N - X_with_sv.shape[1])
    XtX_inv_sva = np.linalg.inv(X_with_sv.T @ X_with_sv)
    se_sva = np.sqrt(mse_sva * XtX_inv_sva[1, 1])
    t_stat_sva = beta_sva[1] / se_sva
    pvals_sva[gene_idx] = 2 * (1 - stats.t.cdf(np.abs(t_stat_sva), N - X_with_sv.shape[1]))

print(f"✓ Completed fitting {n_genes} models")
print(f"\nSummary:")
print(f"  Naive model (no SVA):  {np.sum(pvals_naive < 0.05)} significant genes (p < 0.05)")
print(f"  SVA-adjusted model:    {np.sum(pvals_sva < 0.05)} significant genes (p < 0.05)")
print(f"  True DE genes:         500 (first 500 genes have signal)")
print(f"\nPower improvement: {np.sum(pvals_sva < 0.05) - np.sum(pvals_naive < 0.05)} additional genes detected")


# Create histograms comparing p-value distributions

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Define gene groups
de_genes = np.arange(500)  # First 500 genes have true signal
null_genes = np.arange(500, G)  # Remaining genes are null

# 1. All genes - Naive model
ax = axes[0, 0]
ax.hist(pvals_naive, bins=50, alpha=0.7, color='red', edgecolor='black')
ax.axvline(x=0.05, color='black', linestyle='--', linewidth=2, label='α = 0.05')
ax.set_xlabel('P-value')
ax.set_ylabel('Frequency')
ax.set_title('Naive Model (Without SVA)\nAll {} Genes'.format(G))
ax.legend()
ax.grid(alpha=0.3)

# 2. All genes - SVA model
ax = axes[0, 1]
ax.hist(pvals_sva, bins=50, alpha=0.7, color='green', edgecolor='black')
ax.axvline(x=0.05, color='black', linestyle='--', linewidth=2, label='α = 0.05')
ax.set_xlabel('P-value')
ax.set_ylabel('Frequency')
ax.set_title('SVA-Adjusted Model\nAll {} Genes'.format(G))
ax.legend()
ax.grid(alpha=0.3)

# 3. DE genes only comparison
ax = axes[1, 0]
ax.hist(pvals_naive[de_genes], bins=30, alpha=0.5, color='red', 
        label='Naive', edgecolor='black')
ax.hist(pvals_sva[de_genes], bins=30, alpha=0.5, color='green', 
        label='SVA-adjusted', edgecolor='black')
ax.axvline(x=0.05, color='black', linestyle='--', linewidth=2)
ax.set_xlabel('P-value')
ax.set_ylabel('Frequency')
ax.set_title('True DE Genes Only (n={})'.format(len(de_genes)))
ax.legend()
ax.grid(alpha=0.3)

# 4. Null genes only comparison
ax = axes[1, 1]
ax.hist(pvals_naive[null_genes], bins=30, alpha=0.5, color='red', 
        label='Naive', edgecolor='black')
ax.hist(pvals_sva[null_genes], bins=30, alpha=0.5, color='green', 
        label='SVA-adjusted', edgecolor='black')
ax.axvline(x=0.05, color='black', linestyle='--', linewidth=2)
ax.set_xlabel('P-value')
ax.set_ylabel('Frequency')
ax.set_title('Null Genes Only (n={})'.format(len(null_genes)))
ax.legend()
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('sva_pvalue_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nP-value Histogram Analysis:")
print("="*60)
print(f"\nDE Genes (first 500 with true signal):")
print(f"  Naive:        {np.sum(pvals_naive[de_genes] < 0.05)}/{len(de_genes)} significant ({100*np.sum(pvals_naive[de_genes] < 0.05)/len(de_genes):.1f}%)")
print(f"  SVA-adjusted: {np.sum(pvals_sva[de_genes] < 0.05)}/{len(de_genes)} significant ({100*np.sum(pvals_sva[de_genes] < 0.05)/len(de_genes):.1f}%)")
print(f"  Power gain:   +{np.sum(pvals_sva[de_genes] < 0.05) - np.sum(pvals_naive[de_genes] < 0.05)} genes")

print(f"\nNull Genes (remaining {len(null_genes)} with no signal):")
print(f"  Naive:        {np.sum(pvals_naive[null_genes] < 0.05)}/{len(null_genes)} significant ({100*np.sum(pvals_naive[null_genes] < 0.05)/len(null_genes):.1f}%)")
print(f"  SVA-adjusted: {np.sum(pvals_sva[null_genes] < 0.05)}/{len(null_genes)} significant ({100*np.sum(pvals_sva[null_genes] < 0.05)/len(null_genes):.1f}%)")
print(f"  Expected FPR: ~5% (at α=0.05)")

print(f"\n✓ SVA improves power by {100*(np.sum(pvals_sva[de_genes] < 0.05) - np.sum(pvals_naive[de_genes] < 0.05))/len(de_genes):.1f} percentage points")
print(f"✓ SVA maintains proper FPR control on null genes")



